{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Library\n",
    "\n",
    "We use the following libraries in this project:\n",
    "* [Pandas](https://pandas.pydata.org/) - Data Analysis\n",
    "* [Numpy](https://numpy.org/) - Data Analysis\n",
    "* [Matplotlib](https://matplotlib.org/) - Data Visualization\n",
    "* [Seaborn](https://seaborn.pydata.org/) - Data Visualization\n",
    "* [Scikit-learn](https://scikit-learn.org/stable/) - Machine Learning\n",
    "* [Tensorflow](https://www.tensorflow.org/) - Machine Learning\n",
    "* [Keras](https://keras.io/) - Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "1.23.5\n",
      "0.12.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(sns.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/Vegetable Images'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Directory with our training pictures\n",
    "train_bean_dir = os.path.join(train_dir, 'Bean')\n",
    "train_bitter_gourd_dir = os.path.join(train_dir, 'Bitter_Gourd')\n",
    "train_bottle_gourd_dir = os.path.join(train_dir, 'Bottle_Gourd')\n",
    "train_brinjal_dir = os.path.join(train_dir, 'Brinjal')\n",
    "train_broccoli_dir = os.path.join(train_dir, 'Broccoli')\n",
    "train_cabbage_dir = os.path.join(train_dir, 'Cabbage')\n",
    "train_capsicum_dir = os.path.join(train_dir, 'Capsicum')\n",
    "train_carrot_dir = os.path.join(train_dir, 'Carrot')\n",
    "train_cauliflower_dir = os.path.join(train_dir, 'Cauliflower')\n",
    "train_cucumber_dir = os.path.join(train_dir, 'Cucumber')\n",
    "train_papaya_dir = os.path.join(train_dir, 'Papaya')\n",
    "train_potato_dir = os.path.join(train_dir, 'Potato')\n",
    "train_pumpkin_dir = os.path.join(train_dir, 'Pumpkin')\n",
    "train_radish_dir = os.path.join(train_dir, 'Radish')\n",
    "train_tomato_dir = os.path.join(train_dir, 'Tomato')\n",
    "\n",
    "# Directory with our validation pictures\n",
    "validation_bean_dir = os.path.join(validation_dir, 'Bean')\n",
    "validation_bitter_gourd_dir = os.path.join(validation_dir, 'Bitter_Gourd')\n",
    "validation_bottle_gourd_dir = os.path.join(validation_dir, 'Bottle_Gourd')\n",
    "validation_brinjal_dir = os.path.join(validation_dir, 'Brinjal')\n",
    "validation_broccoli_dir = os.path.join(validation_dir, 'Broccoli')\n",
    "validation_cabbage_dir = os.path.join(validation_dir, 'Cabbage')\n",
    "validation_capsicum_dir = os.path.join(validation_dir, 'Capsicum')\n",
    "validation_carrot_dir = os.path.join(validation_dir, 'Carrot')\n",
    "validation_cauliflower_dir = os.path.join(validation_dir, 'Cauliflower')\n",
    "validation_cucumber_dir = os.path.join(validation_dir, 'Cucumber')\n",
    "validation_papaya_dir = os.path.join(validation_dir, 'Papaya')\n",
    "validation_potato_dir = os.path.join(validation_dir, 'Potato')\n",
    "validation_pumpkin_dir = os.path.join(validation_dir, 'Pumpkin')\n",
    "validation_radish_dir = os.path.join(validation_dir, 'Radish')\n",
    "validation_tomato_dir = os.path.join(validation_dir, 'Tomato')\n",
    "\n",
    "# Directory with our test pictures\n",
    "test_bean_dir = os.path.join(test_dir, 'Bean')\n",
    "test_bitter_gourd_dir = os.path.join(test_dir, 'Bitter_Gourd')\n",
    "test_bottle_gourd_dir = os.path.join(test_dir, 'Bottle_Gourd')\n",
    "test_brinjal_dir = os.path.join(test_dir, 'Brinjal')\n",
    "test_broccoli_dir = os.path.join(test_dir, 'Broccoli')\n",
    "test_cabbage_dir = os.path.join(test_dir, 'Cabbage')\n",
    "test_capsicum_dir = os.path.join(test_dir, 'Capsicum')\n",
    "test_carrot_dir = os.path.join(test_dir, 'Carrot')\n",
    "test_cauliflower_dir = os.path.join(test_dir, 'Cauliflower')\n",
    "test_cucumber_dir = os.path.join(test_dir, 'Cucumber')\n",
    "test_papaya_dir = os.path.join(test_dir, 'Papaya')\n",
    "test_potato_dir = os.path.join(test_dir, 'Potato')\n",
    "test_pumpkin_dir = os.path.join(test_dir, 'Pumpkin')\n",
    "test_radish_dir = os.path.join(test_dir, 'Radish')\n",
    "test_tomato_dir = os.path.join(test_dir, 'Tomato')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many images for each class in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training Bean images: 1000\n",
      "total training Bitter_Gourd images: 1000\n",
      "total training Bottle_Gourd images: 1000\n",
      "total training Brinjal images: 1000\n",
      "total training Broccoli images: 1000\n",
      "total training Cabbage images: 1000\n",
      "total training Capsicum images: 1000\n",
      "total training Carrot images: 1000\n",
      "total training Cauliflower images: 1000\n",
      "total training Cucumber images: 1000\n",
      "total training Papaya images: 1000\n",
      "total training Potato images: 1000\n",
      "total training Pumpkin images: 1000\n",
      "total training Radish images: 1000\n",
      "total training Tomato images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training Bean images:', len(os.listdir(train_bean_dir)))\n",
    "print('total training Bitter_Gourd images:', len(os.listdir(train_bitter_gourd_dir)))\n",
    "print('total training Bottle_Gourd images:', len(os.listdir(train_bottle_gourd_dir)))\n",
    "print('total training Brinjal images:', len(os.listdir(train_brinjal_dir)))\n",
    "print('total training Broccoli images:', len(os.listdir(train_broccoli_dir)))\n",
    "print('total training Cabbage images:', len(os.listdir(train_cabbage_dir)))\n",
    "print('total training Capsicum images:', len(os.listdir(train_capsicum_dir)))\n",
    "print('total training Carrot images:', len(os.listdir(train_carrot_dir)))\n",
    "print('total training Cauliflower images:', len(os.listdir(train_cauliflower_dir)))\n",
    "print('total training Cucumber images:', len(os.listdir(train_cucumber_dir)))\n",
    "print('total training Papaya images:', len(os.listdir(train_papaya_dir)))\n",
    "print('total training Potato images:', len(os.listdir(train_potato_dir)))\n",
    "print('total training Pumpkin images:', len(os.listdir(train_pumpkin_dir)))\n",
    "print('total training Radish images:', len(os.listdir(train_radish_dir)))\n",
    "print('total training Tomato images:', len(os.listdir(train_tomato_dir)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many images for each class in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation Bean images: 200\n",
      "total validation Bitter_Gourd images: 200\n",
      "total validation Bottle_Gourd images: 200\n",
      "total validation Brinjal images: 200\n",
      "total validation Broccoli images: 200\n",
      "total validation Cabbage images: 200\n",
      "total validation Capsicum images: 200\n",
      "total validation Carrot images: 200\n",
      "total validation Cauliflower images: 200\n",
      "total validation Cucumber images: 200\n",
      "total validation Papaya images: 200\n",
      "total validation Potato images: 200\n",
      "total validation Pumpkin images: 200\n",
      "total validation Radish images: 200\n",
      "total validation Tomato images: 200\n"
     ]
    }
   ],
   "source": [
    "print('total validation Bean images:', len(os.listdir(validation_bean_dir)))\n",
    "print('total validation Bitter_Gourd images:', len(os.listdir(validation_bitter_gourd_dir)))\n",
    "print('total validation Bottle_Gourd images:', len(os.listdir(validation_bottle_gourd_dir)))\n",
    "print('total validation Brinjal images:', len(os.listdir(validation_brinjal_dir)))\n",
    "print('total validation Broccoli images:', len(os.listdir(validation_broccoli_dir)))\n",
    "print('total validation Cabbage images:', len(os.listdir(validation_cabbage_dir)))\n",
    "print('total validation Capsicum images:', len(os.listdir(validation_capsicum_dir)))\n",
    "print('total validation Carrot images:', len(os.listdir(validation_carrot_dir)))\n",
    "print('total validation Cauliflower images:', len(os.listdir(validation_cauliflower_dir)))\n",
    "print('total validation Cucumber images:', len(os.listdir(validation_cucumber_dir)))\n",
    "print('total validation Papaya images:', len(os.listdir(validation_papaya_dir)))\n",
    "print('total validation Potato images:', len(os.listdir(validation_potato_dir)))\n",
    "print('total validation Pumpkin images:', len(os.listdir(validation_pumpkin_dir)))\n",
    "print('total validation Radish images:', len(os.listdir(validation_radish_dir)))\n",
    "print('total validation Tomato images:', len(os.listdir(validation_tomato_dir)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many images for eash class in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test Bean images: 200\n",
      "total test Bitter_Gourd images: 200\n",
      "total test Bottle_Gourd images: 200\n",
      "total test Brinjal images: 200\n",
      "total test Broccoli images: 200\n",
      "total test Cabbage images: 200\n",
      "total test Capsicum images: 200\n",
      "total test Carrot images: 200\n",
      "total test Cauliflower images: 200\n",
      "total test Cucumber images: 200\n",
      "total test Papaya images: 200\n",
      "total test Potato images: 200\n",
      "total test Pumpkin images: 200\n",
      "total test Radish images: 200\n",
      "total test Tomato images: 200\n"
     ]
    }
   ],
   "source": [
    "print('total test Bean images:', len(os.listdir(test_bean_dir)))\n",
    "print('total test Bitter_Gourd images:', len(os.listdir(test_bitter_gourd_dir)))\n",
    "print('total test Bottle_Gourd images:', len(os.listdir(test_bottle_gourd_dir)))\n",
    "print('total test Brinjal images:', len(os.listdir(test_brinjal_dir)))\n",
    "print('total test Broccoli images:', len(os.listdir(test_broccoli_dir)))\n",
    "print('total test Cabbage images:', len(os.listdir(test_cabbage_dir)))\n",
    "print('total test Capsicum images:', len(os.listdir(test_capsicum_dir)))\n",
    "print('total test Carrot images:', len(os.listdir(test_carrot_dir)))\n",
    "print('total test Cauliflower images:', len(os.listdir(test_cauliflower_dir)))\n",
    "print('total test Cucumber images:', len(os.listdir(test_cucumber_dir)))\n",
    "print('total test Papaya images:', len(os.listdir(test_papaya_dir)))\n",
    "print('total test Potato images:', len(os.listdir(test_potato_dir)))\n",
    "print('total test Pumpkin images:', len(os.listdir(test_pumpkin_dir)))\n",
    "print('total test Radish images:', len(os.listdir(test_radish_dir)))\n",
    "print('total test Tomato images:', len(os.listdir(test_tomato_dir)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Layer 1\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Layer 2\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Layer 3\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Layer 4\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "\n",
    "    # 15 output neurons for 15 classes with the softmax activation\n",
    "    tf.keras.layers.Dense(15, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18432)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               9437696   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15)                7695      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,542,831\n",
      "Trainable params: 9,542,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 images belonging to 15 classes.\n",
      "Found 3000 images belonging to 15 classes.\n",
      "Found 3000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Rescale all images by 1./255 and apply image augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, batch_size=128, class_mode='categorical', target_size=(224, 224))\n",
    "\n",
    "# Rescale all images by 1./255 and apply image augmentation\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,)\n",
    "\n",
    "# Flow validation images in batches of 128 using validation_datagen generator\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir, batch_size=128, class_mode='categorical', target_size=(224, 224))\n",
    "\n",
    "# Rescale all images by 1./255 and apply image augmentation\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,)\n",
    "\n",
    "# Flow test images in batches of 128 using test_datagen generator\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, batch_size=128, class_mode='categorical', target_size=(224, 224))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(train_generator, \n\u001b[0;32m      2\u001b[0m                     epochs\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, \u001b[39m# changed later \u001b[39;00m\n\u001b[0;32m      3\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, \n\u001b[0;32m      4\u001b[0m                     validation_data\u001b[39m=\u001b[39mvalidation_generator, \n\u001b[0;32m      5\u001b[0m                     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[0;32m      6\u001b[0m                     validation_steps\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, \n",
    "                    epochs=15, # changed later \n",
    "                    steps_per_epoch=20, \n",
    "                    validation_data=validation_generator, \n",
    "                    verbose=1, \n",
    "                    validation_steps=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the test dataset\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('vegetable_classification_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
